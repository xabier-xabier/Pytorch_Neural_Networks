{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQAKEXF0OiUGOhK0fw4pXY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Classification: Cross Entropy Loss\n","Classification attempts to place an item into one of two or more classes. It turns out that many of the tasks we'll be performing in this course are classification tasks. In this lesson, we'll dig in to the cross entropy loss function.\n","\n","When we're performing classification tasks, our outputs for each item look like a vector of scores which we can interpret as probabilities. We are generally comparing this against a vector representing the actual class, where the index of the class is represented by a 1 and all the other indices are 0. We want a loss function that is lower when the score of the correct class is closest to 1. This is what cross entropy loss does. We'll go over cross entropy in detail to understand the loss function, then go over how to use it practically in torch.\n","\n","Cross entropy for a single inference is given by\n","\n","H(p,q)=-(sumatorio de 1 a M)(qi)*(log pi)\n","\n","p=probabilidad\n","q=clase\n","\n","where\n","\n","\\( p \\) is a vector of probabilities, 1 per class\n","\\( p_i \\) is a single probability for one of the output classes\n","\\( q \\) is a one-hot vector labels\n","\\( q_i \\) is the value at a single label\n","\\( M \\) is the number of classes\n","Let's take, for instance, a 3-class task. For a single inference, we will have a softmax-normalized vector we interpret as a probability distribution over the labels that sums to 1.0. For example, let's take the vector \\( [0.1, 0.7, 0.2] \\). We also have our actual distribution that encodes the true label. Let's consider a true distrubution of \\( [0, 1, 0] \\). If we apply the equation, we can see that for indices 0 and 2, \\( q_i = 0 \\), so regardless of what \\( p_i \\) is the output of these indices will be 0. All we really care about is the index of the true label. In this case, the loss for this item will be:\n","\n","\n","\n","Let's use numpy or torch to check our work from the example above. First, let's check that the negative log of 0.7 matches our calculations above."],"metadata":{"id":"ejdrQowSYJ7b"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"sqHabk4pYpc0","executionInfo":{"status":"ok","timestamp":1689673453553,"user_tz":-420,"elapsed":7,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#check our work\n","-np.log(0.7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G74dCYiQYjcm","executionInfo":{"status":"ok","timestamp":1689673455744,"user_tz":-420,"elapsed":10,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"ef925419-2ec8-4cd6-d945-5bd18bf17284"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.35667494393873245"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["Now, let's define a function H that takes in a vector of scores and a vector indicating the correct labels, then apply it to our example case. We'll see that the values we return match."],"metadata":{"id":"tTHCnkXGY3TW"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"J3g5Xhf-aXw4","executionInfo":{"status":"ok","timestamp":1689673905476,"user_tz":-420,"elapsed":5382,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def H(p,q):\n","  return (-1*q*p.log()).sum()"],"metadata":{"id":"v8A9DefEab23","executionInfo":{"status":"ok","timestamp":1689673974137,"user_tz":-420,"elapsed":340,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["#Define a cross entropy function\n","H(torch.tensor([0.1,0.7,0.2]),torch.tensor([0,1,0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRsLnT4WaKQ8","executionInfo":{"status":"ok","timestamp":1689673977108,"user_tz":-420,"elapsed":406,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"348926b2-ef97-44b0-be8e-08b78ce4c95c"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.3567)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["t=torch.tensor([0.1,0.7,0.2])"],"metadata":{"id":"U6XAhQwRa--T","executionInfo":{"status":"ok","timestamp":1689674080396,"user_tz":-420,"elapsed":338,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def our_cross_entropy(yhat,y):\n","  act=yhat[y]\n","  return -act.log()"],"metadata":{"id":"xGy48aN0bGcK","executionInfo":{"status":"ok","timestamp":1689674233404,"user_tz":-420,"elapsed":322,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["our_cross_entropy(t,1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6-ndeSYbsAh","executionInfo":{"status":"ok","timestamp":1689674251140,"user_tz":-420,"elapsed":349,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"90215ae0-a185-4513-b1d2-57ba4d74a513"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.3567)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["In practice, we won't be calculating the loss for single items, but for a batch. To calculate the cross entropy loss for a batch, we just calculate this for each row and take the mean. Just to make it easier, let's just use the indices of the labels instead of the one-hot vectors. Below, we define a function that uses the predicted probabilities, simulated by a normalized tensor of random numbers, and some label indices to calculate the cross entropy loss."],"metadata":{"id":"5bmL3wbHb4IQ"}},{"cell_type":"code","source":["def avg_cross_entropy(yhat,y):\n","  return -yhat[range(y.shape[0]),y].log().mean()    #Echar un vistazo, no lo veo"],"metadata":{"id":"VRMH0jFSb5uA","executionInfo":{"status":"ok","timestamp":1689674431861,"user_tz":-420,"elapsed":312,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["t=torch.randn(3,3).softmax(dim=1)\n","t"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmfdwuAvccfK","executionInfo":{"status":"ok","timestamp":1689674469185,"user_tz":-420,"elapsed":13,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"1e01295d-6b8f-4995-d3a5-474e0f871ec8"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.7502, 0.0828, 0.1670],\n","        [0.3724, 0.2735, 0.3541],\n","        [0.0895, 0.8934, 0.0171]])"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["y=torch.randint(low=0, high=3, size=(3,))\n","y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-aWymW5IcqYB","executionInfo":{"status":"ok","timestamp":1689674526158,"user_tz":-420,"elapsed":6,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"c5a0bb7b-724d-46f1-8edb-754d4e1c752b"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 2, 0])"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["avg_cross_entropy(t,y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUqgyTU7c2w-","executionInfo":{"status":"ok","timestamp":1689674562549,"user_tz":-420,"elapsed":342,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"bf7e997e-e073-4716-e9d2-7538d2f8bbd8"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.2463)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["One common problem in deep learning is that of precision. Here, we're not talking about the ML metric - we're talking about the computer's limited ability to store really large or really small numbers in memory. For each value we work with, torch will allocate a specific amount of memory, and there's a limit to how close or far from zero for numbers we can store within that memory. If we are working numbers near those limits, performing operations on those numbers are often not precise because we just can't store an accurate representation of the result. Let's multiply some small and large numbers to illustrate this."],"metadata":{"id":"IWdK-P-gd5EJ"}},{"cell_type":"code","source":["a = 0.00000000000000000000000000001\n","a * a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZaL4ON3d8at","executionInfo":{"status":"ok","timestamp":1689674831087,"user_tz":-420,"elapsed":342,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"38cedfc5-d092-47ca-ef27-a1f1836956df"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.999999999999998e-59"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["a = 100000000000000000000000000000.\n","a * a"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ts1ZwudBd--l","executionInfo":{"status":"ok","timestamp":1689674841221,"user_tz":-420,"elapsed":345,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"ece374dc-02a5-4dd1-c0d5-8a2c4a7dae0d"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.999999999999998e+57"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["In deep learning, it's common to work with very small or very large numbers. When we multiply small numbers together, they get even smaller and closer to that limit of precision. There is a useful property of logs/exponents that helps us calculate these numbers a bit more precisely:\n","\n","log(a*b)=log(a)+log(b)\n","a*b=e(log(a)+log(b))\n","\n","\n","Let's revisit the softmax function that we apply to turn the logits (outputs of our final layer) into a probability distribution. Given a vector of logits \\( z \\) :\n","\n","sigma(z)=(e_elevado(zi))/(sumatorio 1 a N de e_elevado(zi))\n","\n","\n","Between softmax and cross entropy, there are a lot of logs and exponents happening, Without going into too much detail, torch's implementation of cross entropy loss combines the softmax and cross entropy loss functions in a way that's more efficient than calculating it with normalized to a probability distribution. For a more detailed explanation of cross entropy loss and torch's implementation, check out this video.\n","\n","Let's see how cross entropy loss works in practice."],"metadata":{"id":"NtzHxp0deC0O"}},{"cell_type":"code","source":["def make_classification_logits(n_classes, n_samples, pct_correct, confidence=1):\n","    \"\"\"\n","    This function returns simulated logits and classes.\n","\n","    n_classes: nuber of classes\n","    n_samples: number of rows\n","    pct_correct: float between 0 and 1. The higher it is,\n","                 the higher the % of logits that will\n","                 generate the correct output.\n","    confidence: controls how confident our logits are.\n","                Closer to 0: less confident\n","                Larger: more confident\n","    \"\"\"\n","    classes = list(range(n_classes))\n","    # Randomly make logits\n","    logits = np.random.uniform(-5., 5., (n_samples, n_classes))\n","    # Randomly make labels\n","    labels = np.random.choice(classes, size=(n_samples))\n","    # Find the max of each row in logits\n","    maxs = np.abs(logits).max(axis=1)\n","    # For each row...\n","    for i in range(len(maxs)):\n","        # If we want the answer to be right...\n","        if np.random.random() <= pct_correct:\n","            # Make the correct item the highest logit\n","            logits[i, labels[i]] = maxs[i] + np.random.random()*confidence\n","        # If we want it to be wrong...\n","        else:\n","            # Make the highest logit a different index\n","            _c = classes.copy()\n","            _c.remove(classes[labels[i]])\n","            _i = np.random.choice(_c)\n","            logits[i, _i] = maxs[i] + np.random.random()/10\n","\n","    # Return logits and labels\n","    return torch.FloatTensor(logits), torch.tensor(labels)"],"metadata":{"id":"6eFJdxDvefUX","executionInfo":{"status":"ok","timestamp":1689675044186,"user_tz":-420,"elapsed":6,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["Let's use the function defined above to make some classification logits and associated labels."],"metadata":{"id":"V6mxkrjYe14y"}},{"cell_type":"code","source":["# Create some logits and associated labels.\n","# There will be some error here!\n","logits, labels = make_classification_logits(3, 10, 0.8, confidence=1)\n","logits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jyns4ZZze3QP","executionInfo":{"status":"ok","timestamp":1689675080338,"user_tz":-420,"elapsed":331,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"b0633aa5-930a-4ca3-b720-8cf7a3f67e2d"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 4.2177, -4.0934,  0.0668],\n","        [-0.7385,  2.6636,  3.4101],\n","        [-4.0501,  1.9145,  4.6214],\n","        [ 4.8040,  1.1541, -2.3394],\n","        [ 2.5674, -2.1252,  1.5663],\n","        [-2.0453,  3.3044,  0.0779],\n","        [ 4.1436,  5.1625,  4.5696],\n","        [ 4.2516,  3.3550, -0.4895],\n","        [-0.7235, -0.0780,  2.7425],\n","        [-4.7485,  0.8806,  5.0635]])"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["Let's use the .softmax method to normalize the logits to a probability score."],"metadata":{"id":"byjbdDnxe8-H"}},{"cell_type":"code","source":["# What are the normalized predicted probabilities for each class?\n","logits.softmax(dim=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DM29mwthe_rg","executionInfo":{"status":"ok","timestamp":1689675106978,"user_tz":-420,"elapsed":383,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"83dee2af-f5bc-49e9-d319-130b9671e840"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[9.8426e-01, 2.4190e-04, 1.5502e-02],\n","        [1.0596e-02, 3.1819e-01, 6.7122e-01],\n","        [1.6065e-04, 6.2557e-02, 9.3728e-01],\n","        [9.7392e-01, 2.5315e-02, 7.6946e-04],\n","        [7.2642e-01, 6.6558e-03, 2.6692e-01],\n","        [4.5471e-03, 9.5745e-01, 3.8007e-02],\n","        [1.8863e-01, 5.2255e-01, 2.8882e-01],\n","        [7.0587e-01, 2.8797e-01, 6.1616e-03],\n","        [2.8639e-02, 5.4614e-02, 9.1675e-01],\n","        [5.3965e-05, 1.5024e-02, 9.8492e-01]])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Let's look find the indices with the highest probability. These will serve as our predictions."],"metadata":{"id":"tv9TrzgDfDEH"}},{"cell_type":"code","source":["# How well do they match with our labels?\n","labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6lvWO9JfGTG","executionInfo":{"status":"ok","timestamp":1689675134705,"user_tz":-420,"elapsed":340,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"e0fd59b5-f3b3-402d-e621-f923937f586c"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 2, 2, 1, 0, 1, 1, 0, 1, 2])"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","source":["Now let's use our logits to calculate cross entropy loss for this entire batch of data using nn.CrossEntropyLoss() and F.cross_entropy. These functions take in the logits and labels, and return the cross entroypy loss for the batch."],"metadata":{"id":"SS02JPTrfJp-"}},{"cell_type":"code","source":["from torch import nn"],"metadata":{"id":"21z6Lh14fNwf","executionInfo":{"status":"ok","timestamp":1689675179857,"user_tz":-420,"elapsed":3,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["cross_entropy = nn.CrossEntropyLoss()"],"metadata":{"id":"OiWVysP2fKz3","executionInfo":{"status":"ok","timestamp":1689675183312,"user_tz":-420,"elapsed":4,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["cross_entropy(logits, labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zEBxCKLFfVK4","executionInfo":{"status":"ok","timestamp":1689675194833,"user_tz":-420,"elapsed":5,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"ca338805-0a9f-4323-e975-56cc49f3eba0"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.8439)"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["import torch.nn.functional as F"],"metadata":{"id":"3nTi_X14feLA","executionInfo":{"status":"ok","timestamp":1689675247863,"user_tz":-420,"elapsed":4,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["F.cross_entropy(logits, labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"svIlsUzUfXqY","executionInfo":{"status":"ok","timestamp":1689675249830,"user_tz":-420,"elapsed":414,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"e015888d-dac7-445c-c6a6-7b0e4f59d9ff"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.8439)"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["Finally, let's apply our cross entropy loss function from earlier over this batch and make sure we get the same result."],"metadata":{"id":"DfTzDI5xfmk0"}},{"cell_type":"code","source":["torch.mean(\n","    torch.tensor(\n","        [our_cross_entropy(lo, la)\n","         for lo, la # softmax of logits, labels\n","         in zip(logits.softmax(dim=1), labels)\n","        ]\n","    )\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CxolzLxffnus","executionInfo":{"status":"ok","timestamp":1689675277496,"user_tz":-420,"elapsed":346,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"00a316d5-dc9d-49a4-87ee-cc2ae359b2cc"},"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.8439)"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["In this lesson, we reviewed cross entropy loss, the general loss function for classificaiton. We learned a little bit about numerical precision, and how multiplying small or large numbers together can result in some numerical error. This is why we use the logits to calculate the loss instead of the normalized probabilities."],"metadata":{"id":"ItinqW9Bfr-0"}}]}