{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOllljh6sRyO8+gRQP0UDbF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Classification: Multicategory Classification\n","\n","Sometimes, you may want to make many mutually independent decisions using the same model. In this scenario, your model will return many outputs, each output corresponding to the binary probability of a different category, so the probabilities for an item may sum to more than 1.0.\n","\n","Let's say we want to build a model to analyze the sentiment of tweets about the NBA. We could have two models, one for sentiment (1 is positive, 0 is negative) and another classifying whether it's about the NBA. Alternately, we could just use the same model for both. We could use one output for whether it's positive or negative (sentiment), and another output for whether it's about the NBA; the probability of one shouldn't be dependent on the other.\n","\n","Let's simulate some potential logits:"],"metadata":{"id":"OfOIyh62RE7i"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"4Rpnd3rZRhhr","executionInfo":{"status":"ok","timestamp":1689688361732,"user_tz":-420,"elapsed":7103,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#Column 0 may be the p(happy)=1-p(sad)\n","#and column 2 may be p(lakers)=1-p(not lakers)\n","\n","logits=torch.rand(10,2)*2\n","logits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_YlPr6WRHUq","executionInfo":{"status":"ok","timestamp":1689688367740,"user_tz":-420,"elapsed":533,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"47a56f0f-2926-429f-d425-37cd1b03a153"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.6969, 0.9234],\n","        [1.5039, 0.8510],\n","        [0.9432, 1.1963],\n","        [0.5297, 0.6372],\n","        [0.5523, 0.8420],\n","        [0.0046, 0.3863],\n","        [1.9631, 0.9802],\n","        [1.5399, 1.0572],\n","        [0.1167, 0.9180],\n","        [1.5522, 0.3613]])"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["Now, we need labels for these logits. We'll need one column of labels per column of logits."],"metadata":{"id":"V-wZyXDJRosh"}},{"cell_type":"code","source":["labels=torch.randint(0,2,(10,2)).float()\n","labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OZqK9ayMRqc5","executionInfo":{"status":"ok","timestamp":1689688417483,"user_tz":-420,"elapsed":7,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"2d2ee5f7-f16a-40cb-a3f2-7fa2d0e8bb7e"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0.],\n","        [1., 0.],\n","        [0., 1.],\n","        [1., 1.],\n","        [1., 1.],\n","        [0., 1.],\n","        [0., 1.],\n","        [1., 0.],\n","        [0., 1.],\n","        [1., 0.]])"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["To convert to probabilities, we still need to normalize. But we no longer need the probabilities of each row to add up to 1.0, since they are not mutually exclusive. Notice we normalize each value independently."],"metadata":{"id":"dOvYTD7rR3EA"}},{"cell_type":"code","source":["logits.sigmoid()    #no need of dimension as mentioned above"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51-ovi_fR7Wn","executionInfo":{"status":"ok","timestamp":1689688464051,"user_tz":-420,"elapsed":378,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"1374ec0a-8eac-4ffe-dea3-4ffc0d8bedc9"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8451, 0.7157],\n","        [0.8182, 0.7008],\n","        [0.7198, 0.7679],\n","        [0.6294, 0.6541],\n","        [0.6347, 0.6989],\n","        [0.5012, 0.5954],\n","        [0.8769, 0.7271],\n","        [0.8234, 0.7422],\n","        [0.5291, 0.7146],\n","        [0.8252, 0.5893]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["We can still use F.binary_cross_entropy_with_logits or nn.BCEWithLogitsLoss() to calculate the loss here. It will reduce the loss over all the logits and labels for us."],"metadata":{"id":"EiOzcKovSEXI"}},{"cell_type":"code","source":["from torch import nn"],"metadata":{"id":"jrc8uGGjSMJ_","executionInfo":{"status":"ok","timestamp":1689688537849,"user_tz":-420,"elapsed":5,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["bce=nn.BCEWithLogitsLoss()"],"metadata":{"id":"7UsgIWPxSHN3","executionInfo":{"status":"ok","timestamp":1689688573887,"user_tz":-420,"elapsed":5,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["bce(logits,labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYr2IimOSZXh","executionInfo":{"status":"ok","timestamp":1689688588077,"user_tz":-420,"elapsed":6,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"3627d966-559a-49dc-a11b-8d227ab50e22"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.6709)"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["Let's perform a sanity check by calculating the binary cross entropy for each column separately. The mean of these two values should be the same as the output from the code above."],"metadata":{"id":"izo5W2eESeYa"}},{"cell_type":"code","source":["bce(logits[:,1],labels[:,1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4btBFtgiShHP","executionInfo":{"status":"ok","timestamp":1689688633382,"user_tz":-420,"elapsed":347,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"fb87f8cc-e7db-429a-ecbf-2cbb542eb510"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.6930)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["bce(logits[:,0],labels[:,0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlAWdgesSqd4","executionInfo":{"status":"ok","timestamp":1689688676316,"user_tz":-420,"elapsed":380,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"9db2dc9c-283d-4339-9220-d209d09d74c1"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.6488)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["(bce(logits[:,0],labels[:,0])+bce(logits[:,1], labels[:,1]))/2  #It must match the previous calc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DEZggH-KSyiR","executionInfo":{"status":"ok","timestamp":1689688738466,"user_tz":-420,"elapsed":6,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"e7162c8a-93e4-4c6c-b81c-8da107c69b12"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.6709)"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["In this lesson, we reviewed the case for multilabel classification, and how to apply binary cross entropy to build models that output more than one decision at a time."],"metadata":{"id":"GktXBlxRTHeo"}}]}