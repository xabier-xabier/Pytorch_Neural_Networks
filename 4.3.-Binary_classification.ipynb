{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNsHFhFnfacAoqXwnWsChxX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Classification: Binary classification\n","\n","Now that we've gone over multiclass classification, we'll take a look at binary classification. This can be a bit more confusing since there is more than one way to accomplish this. We'll take a look at a few ways to calculate binary cross entropy.\n","\n","The first option is to have 2 logits output by the model: 1 for the negative class, and one for the positive, shown below. This is the same workflow as the multiclass scenario from the previous lesson."],"metadata":{"id":"C25lxLFmrRmo"}},{"cell_type":"code","source":["import numpy as np\n","import torch"],"metadata":{"id":"PKKHSuttr-td","executionInfo":{"status":"ok","timestamp":1689687722253,"user_tz":-420,"elapsed":4329,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["What Are Logits? In machine learning, the term “logits” refers to the raw outputs of a model before they are transformed into probabilities. Specifically, logits are the unnormalized outputs of the last layer of a neural network."],"metadata":{"id":"_n99SQn9u6Ed"}},{"cell_type":"code","source":["def make_classification_logits(n_classes, n_samples, pct_correct, confidence=1):\n","    \"\"\"\n","    This function returns simulated logits and classes.\n","\n","    n_classes: nuber of classes\n","    n_samples: number of rows\n","    pct_correct: float between 0 and 1. The higher it is,\n","                 the higher the % of logits that will\n","                 generate the correct output.\n","    confidence: controls how confident our logits are.\n","                Closer to 0: less confident\n","                Larger: more confident\n","    \"\"\"\n","    classes = list(range(n_classes))\n","    # Randomly make logits\n","    logits = np.random.uniform(-5., 5., (n_samples, n_classes))\n","    # Randomly make labels\n","    labels = np.random.choice(classes, size=(n_samples))\n","    # Find the max of each row in logits\n","    maxs = np.abs(logits).max(axis=1)       #In numpy.max axis=0 is column and axis=1 is row\n","    # For each row...\n","    for i in range(len(maxs)):\n","        # If we want the answer to be right...\n","        if np.random.random() <= pct_correct:   #np.random.random(), Return random floats in the half-open interval [0.0, 1.0). Alias for random_sample to ease forward-porting to the new random API.\n","            # Make the correct item the highest logit\n","            logits[i, labels[i]] = maxs[i] + np.random.random()*confidence\n","        # If we want it to be wrong...\n","        else:\n","            # Make the highest logit a different index\n","            _c = classes.copy()\n","            _c.remove(classes[labels[i]])\n","            _i = np.random.choice(_c)\n","            logits[i, _i] = maxs[i] + np.random.random()/10\n","\n","    # Return logits and labels\n","    return torch.FloatTensor(logits), torch.tensor(labels)"],"metadata":{"id":"fJXTidz6r7qt","executionInfo":{"status":"ok","timestamp":1689687727800,"user_tz":-420,"elapsed":5,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["logits, labels=make_classification_logits(2,10,1., confidence=1)"],"metadata":{"id":"jG2jzqucrU9m","executionInfo":{"status":"ok","timestamp":1689687732103,"user_tz":-420,"elapsed":4,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["np.abs(logits).max(axis=1) #just to check it. It`s taking max values per row."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FmSnwBN9PLtM","executionInfo":{"status":"ok","timestamp":1689687742390,"user_tz":-420,"elapsed":925,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"14a2494f-153c-48d0-d24f-daa88a2e68b8"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.return_types.max(\n","values=tensor([4.3765, 2.9773, 4.7288, 3.8919, 4.6187, 3.5896, 5.0186, 2.5354, 5.3304,\n","        3.9719]),\n","indices=tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0]))"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["logits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVf5CAHnsI_t","executionInfo":{"status":"ok","timestamp":1689687750370,"user_tz":-420,"elapsed":445,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"4ebc67d0-4ebe-4c02-d03b-451a74e0564e"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 4.3765, -3.3314],\n","        [ 2.9773,  2.2280],\n","        [ 4.7288,  4.6093],\n","        [ 3.8919, -2.4916],\n","        [ 4.6187, -4.1310],\n","        [ 1.0369,  3.5896],\n","        [ 5.0186, -2.1783],\n","        [ 2.0699,  2.5354],\n","        [ 1.7808,  5.3304],\n","        [ 3.9719,  3.2910]])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["logits.softmax(dim=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"70ji5KWJsUGP","executionInfo":{"status":"ok","timestamp":1689687834549,"user_tz":-420,"elapsed":549,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"8399073e-982b-445a-cc1b-9b9be9f8397e"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[9.9955e-01, 4.4908e-04],\n","        [6.7904e-01, 3.2096e-01],\n","        [5.2984e-01, 4.7016e-01],\n","        [9.9831e-01, 1.6864e-03],\n","        [9.9984e-01, 1.5849e-04],\n","        [7.2246e-02, 9.2775e-01],\n","        [9.9925e-01, 7.4831e-04],\n","        [3.8567e-01, 6.1433e-01],\n","        [2.7935e-02, 9.7206e-01],\n","        [6.6395e-01, 3.3605e-01]])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tq3dl8i4sZd1","executionInfo":{"status":"ok","timestamp":1689687830544,"user_tz":-420,"elapsed":359,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"5097854c-1193-4a78-918d-f5059a5d39b4"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["import torch.nn.functional as F"],"metadata":{"id":"gdABsuvJscSP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["F.cross_entropy(logits,labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"njZEi63OsmMV","executionInfo":{"status":"ok","timestamp":1689678685079,"user_tz":-420,"elapsed":303,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"e473f756-bf8c-4566-b183-cca6330c30b0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.1623)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["The second option is to organize your model to have only 1 output unit. In that case, you can use binary cross entropy as your loss function. Let's see a small example. In the cell below, we will make some logits and some labels. In this case, each number in the logits and labels arrays represent a single item."],"metadata":{"id":"IQApuq2QsuiB"}},{"cell_type":"code","source":["#Make some logits and labels, making sure not to get everything correct\n","logits=torch.tensor([2.8,-1.4,1.1,-.8])\n","labels=torch.tensor([1.,0.,0.,0.])"],"metadata":{"id":"R9BI5JxIswNF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#view the logits\n","logits"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwInm3AKtCf9","executionInfo":{"status":"ok","timestamp":1689678796076,"user_tz":-420,"elapsed":4,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"fa2f6f07-22f6-422b-b99a-14fded38dc20"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 2.8000, -1.4000,  1.1000, -0.8000])"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["Let's take a look at the normalized probability scores for each item. Since each value represents an individual item, we no longer need to pass a dim to the softmax method."],"metadata":{"id":"ML4SkSvwtJNt"}},{"cell_type":"code","source":["#Normalize to probabilities\n","logits.sigmoid()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_h3lCOTtKqd","executionInfo":{"status":"ok","timestamp":1689678841699,"user_tz":-420,"elapsed":6,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"a1fa1112-6d31-435d-95f2-861f4654008b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.9427, 0.1978, 0.7503, 0.3100])"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["Finally, let's use some torch methods to calculate the binary cross entropy loss."],"metadata":{"id":"bRFeXmV-tVtl"}},{"cell_type":"code","source":["#Calculate loss with the logits\n","F.binary_cross_entropy_with_logits(logits,labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QO-LWqNItXEc","executionInfo":{"status":"ok","timestamp":1689678906968,"user_tz":-420,"elapsed":353,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"3fd07a06-8652-42c7-f6b8-21aaa8901368"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.5095)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from torch import nn"],"metadata":{"id":"YfDApDi2ttHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calculate the loss with the logits\n","nn.BCEWithLogitsLoss()(logits,labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wS_1xVgith21","executionInfo":{"status":"ok","timestamp":1689678976921,"user_tz":-420,"elapsed":5,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"0539c3a8-edbe-40c8-d199-9ab72c4a3cf1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.5095)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["We can also calculate the loss with the probabilities. But here we can see that we don't get exactly the same numbers. This illustrates why calcuilating cross entropy with logits is more numerically stable than with the normalized probabilities."],"metadata":{"id":"1sRMpwfztzYd"}},{"cell_type":"code","source":["F.binary_cross_entropy(logits.sigmoid(),labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJQIufLQt2AU","executionInfo":{"status":"ok","timestamp":1689679028988,"user_tz":-420,"elapsed":4,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"1fb4728b-68ec-46bf-ba68-d16e60fe622f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.5095)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["nn.BCELoss()(logits.sigmoid(),labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ppZnM5v4t-8V","executionInfo":{"status":"ok","timestamp":1689679055444,"user_tz":-420,"elapsed":411,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"c8032473-1fa8-45cc-aa7c-af32a3c27e5d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.5095)"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["We can see that the values for the binary cross entropy loss using logits and labels are close using the sanity check below."],"metadata":{"id":"i85FNzfWuGI1"}},{"cell_type":"code","source":["# Values are close...\n","torch.allclose(F.binary_cross_entropy_with_logits(logits, labels), F.binary_cross_entropy(logits.sigmoid(), labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRD2KReBuIAk","executionInfo":{"status":"ok","timestamp":1689679088837,"user_tz":-420,"elapsed":307,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"2bf329c4-14fb-4c78-a0f4-305f7403e145"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["However, the code below indicates that the loss calcualted with the logits and the loss calculated with the normalized probabilities are not in fact identical."],"metadata":{"id":"f_X85QX7uPPm"}},{"cell_type":"code","source":["# ... but not the same\n","F.binary_cross_entropy_with_logits(logits, labels) == F.binary_cross_entropy(logits.sigmoid(), labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVYMEdGquTWl","executionInfo":{"status":"ok","timestamp":1689679119990,"user_tz":-420,"elapsed":325,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"c0b051c0-bb65-4767-a30e-b9a117db0249"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(True)"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["Finally, let's calculate the difference between the outputs of the two loss functions."],"metadata":{"id":"7PEUKK8EuW4F"}},{"cell_type":"code","source":["F.binary_cross_entropy_with_logits(logits, labels) - F.binary_cross_entropy(logits.sigmoid(), labels)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fj0PZZUnuYWl","executionInfo":{"status":"ok","timestamp":1689679149280,"user_tz":-420,"elapsed":327,"user":{"displayName":"Xabi Sagarzazu","userId":"11788458476726030662"}},"outputId":"789af8ea-780a-4d85-8a50-37a5bf4d1c9b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.)"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["In this lesson, we reviewed binary cross entropy loss in depth. We also saw that calculating the loss with logits can be more precise than calculating the loss with the normalized probabilities."],"metadata":{"id":"B8Zf39n6udYE"}}]}